{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from __future__ import division\n",
    "#rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "#rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mid-term exam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "\n",
    "Consider the joint density $f(x, u) = 1(0<u<\\pi(x))$, the conditional density should be:\n",
    "\n",
    "i. given x, u should satisfy a uniform distribution $U(0, \\pi(x))$\n",
    "\n",
    "ii. given u, x should satisfy a uniform distribution on $A_u = \\{x:\\pi(x) > u \\}$\n",
    "\n",
    "Thus, the algorithm of the Gibbs sampler:\n",
    "\n",
    "1. sample $x_0$ from a proposed initial density $q_0(x)$\n",
    "2. sample u from $U(0, \\pi(x))$\n",
    "3. sample x from $U(A_u)$\n",
    "4. iterate 2 and 3 until convergence is reached and enough samples are generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, to sample $\\pi(x) \\propto 1/(1+x^4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pi(x):\n",
    "    return 1/(1+x**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above algorithm and joint density, $u|_x \\sim U(0, 1/(1+x^4))$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_u(x):\n",
    "    u = np.random.uniform()\n",
    "    while u >= pi(x):\n",
    "        u = np.random.uniform()\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And $x|_u ~ U(-\\pi^{-1}(u),\\pi^{-1}(u))$, where $\\pi^{-1}(u)$ is defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi^{-1}(u) = (\\frac{1}{u} - 1)^{1/4}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_x(u):\n",
    "    x = np.random.uniform(-pow(1/u-1, 1/4), pow(1/u-1, 1/4))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this Gibbs sampler to generate a million samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b9818a074ed5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-028c8375d8ad>\u001b[0m in \u001b[0;36mdraw_u\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "u = np.random.uniform()\n",
    "N = 1000000\n",
    "samples = np.zeros(N)\n",
    "for i in range(N):\n",
    "    x = draw_x(u)\n",
    "    samples[i] = x\n",
    "    u = draw_u(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram of histogram of the samples, tt can be shown that this is consistent with the actual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(samples, kde=False, norm_hist=True, label=\"histogram\")\n",
    "x = np.linspace(-35, 35,1000)\n",
    "plt.plot(x, 1/(1+x**4) * np.sqrt(2) / np.pi, label='\\pi(x)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Monte Carlo integral for $I = \\int x^2 \\pi(x) dx$, plot the convergence curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = samples**2\n",
    "for i in range(1, N):\n",
    "    I[i] = (I[i-1]*i + samples[i]**2)/(i+1) #fast, probably unstable way to compute average vs iteration\n",
    "    \n",
    "plt.plot(range(N), I)\n",
    "plt.xlabel(\"number of iterations\")\n",
    "plt.ylabel(\"I\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the first 200000 iterations as a burn-in simulation and compute the MC approximation $\\hat{I}_N = \\sum_{i}^n x_i y_i / n$ to estimate variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(samples**2)[200000:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I assume $\\pi(x)$ is a probability function. Thus this MC integration is close to the actual integration which is:\n",
    "\n",
    "\\begin{equation}\n",
    "I = \\frac{\\int x^2 \\pi(x) dx}{\\int \\pi(x) dx} = 1\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "From Bayes' theory:\n",
    "\n",
    "\\begin{align}\n",
    "f(\\mu, \\lambda \\ | \\ data) &\\propto \\prod_{i=1}^n g(x_i \\ | \\ \\mu, \\lambda) \\cdot f(\\mu) \\cdot f(\\lambda) \\\\\n",
    "                        &=  \\lambda^{n/2} e^{\\lambda \\sum_{i=1}^n -(x_i-\\mu)^2 / 2} \\cdot \\frac{1}{ \\tau} e^{-\\mu^2 / 2\\tau^2} \\cdot \\lambda^{a-1} e^{-\\lambda b}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the size of the samples is rather small, I picked larg $\\tau$ and small a and b so that the Markov chain is less sensitive to the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tau = 10; a = 2; b = 5\n",
    "n = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearange $f(\\mu, \\lambda \\ | \\ data)$ and get conditional densities for $\\mu$ and $\\lambda$:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mu | \\lambda, data) &\\propto e^{\\lambda \\sum_{i=1}^n -(x_i-\\mu)^2 / 2} \\cdot  e^{-\\mu^2 / 2\\tau^2}\\\\\n",
    "                        &\\propto e^{-\\frac{1}{2} (\\mu - \\frac{\\lambda n \\bar{x}}{1/\\tau^2 + n \\lambda})^2 \\cdot (1/\\tau^2 + n \\lambda)}\n",
    "\\end{align}\n",
    "\n",
    "$\\mu |_\\lambda \\sim N(\\frac{\\lambda n \\bar{x}}{1/\\tau^2 + n \\lambda}, \\frac{1}{1/\\tau^2 + n \\lambda})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_mu(lamda):\n",
    "    return np.random.normal(lamda*10.2/(1/tau**2+n*lamda), np.sqrt(1/(n*lamda+1/tau**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "p(\\lambda | \\mu , data) \\propto \\lambda^{n/2 + a -1} e^{-\\frac{1}{2} \\lambda [b+\\sum_i(x_i-\\mu)^2]}\n",
    "\\end{align}\n",
    "\n",
    "$\\lambda|_\\mu \\sim Gamma(n/2+a, \\frac{b+\\sum_i (x_i-\\mu)^2}{2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_lamda(mu):\n",
    "    return np.random.gamma(shape = a+n/2, scale=2/(b+(39.6 -2*mu*10.2 + n*mu**2) - 10.2**2/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous question, the Gibbs sampler starts with an initial $\\mu_0$, then sample $\\lambda$ and $\\mu$ iteratively based on the above two conditional densities in a round Robin fashion (100000 samples were generated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.random.normal(0.2, 0.751)\n",
    "N2 = 100000\n",
    "samples2 = np.zeros((N2, 2))\n",
    "for i in range(N2):\n",
    "    lamda = draw_lamda(mu)\n",
    "    mu = draw_mu(lamda)\n",
    "    samples2[i][0] = mu\n",
    "    samples2[i][1] = lamda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the marginal densities for $\\mu$:\n",
    "\\begin{align}\n",
    "p(\\mu | data) &\\propto \\frac{f(\\mu, \\lambda \\ | \\ data)}{p(\\lambda | \\mu , data)} \\\\\n",
    "                &\\propto \\exp(-\\mu^2 / 2\\tau^2)\\cdot [b+ \\frac{\\sum_i(x_i-\\mu)^2}{2}]^{-(a+n/2)}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-0.5, 0.9, 1001)\n",
    "y = np.exp(-x**2/2/tau**2)*(b + (39.6 - 2*10.2*x + n*x**2)/2)**(-a-n/2)\n",
    "plt.plot(x,y, label = r\"p(\\mu)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample distribution for $\\mu$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(samples2[:,0], norm_hist=True, label=\"histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal density of $\\lambda$:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\lambda | data) &\\propto \\propto \\frac{f(\\mu, \\lambda \\ | \\ data)}{p(\\mu \\ | \\ \\lambda , data)} \\\\\n",
    "                    &\\propto \\lambda^{n/2+a-1} e^{-\\lambda b} \\sqrt{1/\\tau^2 + n\\lambda} e^{-\\frac{1}{2} \n",
    "                    [\\lambda \\sum_{i=1}^n x_i^2 - \\frac{(\\sum_i x_i \\lambda)^2}{n \\lambda + 1/\\tau^2}]}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,3,1001)\n",
    "y = x**(n/2+a-1)*np.exp(-x*b) * np.sqrt(1/tau**2+n*x) * np.exp((10.2*x)**2/(n*x+1/tau**2)/2 - x*39.6/2)\n",
    "plt.plot(x, y)\n",
    "plt.xlim(0,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample distribution of $\\lambda$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(samples2[:,1])\n",
    "plt.xlim(0,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample distribution is consistent with the real marginal distribution.\n",
    "\n",
    "With this sampler, we can estimate the value of $\\mu$ and $\\lambda$ by taking the sample mean:\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{\\mu} = \\frac{1}{N} \\sum_i^N \\mu_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{\\lambda} = \\frac{1}{N} \\sum_i^N \\lambda_i\n",
    "\\end{equation}\n",
    "\n",
    "Now plot the convergence curve of $\\mu$ and $\\lambda$ respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = samples2[:,0].copy()\n",
    "lamda = samples2[:,1].copy()\n",
    "\n",
    "for i in range(1, N2):\n",
    "    mu[i] = (mu[i-1]*i + samples2[i, 0])/(i+1)\n",
    "    lamda[i] = (lamda[i-1]*i + samples2[i,1])/(i+1)\n",
    "    \n",
    "plt.plot(range(N2), mu)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(N2), lamda)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples2[-20000:].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bar{\\mu}$ and $\\bar{\\lambda}$ from sampling the posterior with the Gibbs sampler are 0.2012 and 1.329 respectively. The mean of the data is $10.2/51 = 0.2$ and inverse variance of the data is $1 / \\frac{1}{n-1}\\sum_i (x_i - \\bar{x})^2 = 1/[(39.6 - 2\\cdot0.2\\cdot10.2 + n\\cdot0.2^2)/(n-1)] = 1.3312$. Thus the estimates agree with the sample mean and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha3 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the power law distribution: $P(X=k)\\propto k^{-\\alpha}, \\ k \\in \\{1, 2, 3, ... \\}, \\alpha > 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pi(k):\n",
    "    return 1/np.power(k, alpha3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposal for Metropolis-Hasting algorithm (1D random walk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q(x2, x1):\n",
    "    assert abs(x2-x1) == 1 #check is the two states are next to each other\n",
    "    if x2 == 2 and x1 == 1:\n",
    "        return 1\n",
    "    return 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\alpha (x_2, x_1)$ for Metropolis-Hasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha(x2, x1):\n",
    "    \n",
    "    return min(1, pi(x2) / pi(x1) * q(x1, x2) / q(x2, x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm for Metropolis-Hasting:\n",
    "1. Start from a $x_1$\n",
    "2. sample $x_2^*$ from proposal probability\n",
    "3. sample u from U(0,1)\n",
    "4. accept $x_2^*$ if $u < \\alpha(x_2^*, x_1)$, else stay at $x_1$\n",
    "5. iterate through 2-4 until enough samples are acquired.\n",
    "\n",
    "For this problem, we start the Markov chain at 1 and iterate 10000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = 1\n",
    "N3 = 10000\n",
    "samples3 = np.zeros(N3)\n",
    "for i in range(N3):\n",
    "    x2_tmp = 2 if x1 == 1 else np.random.choice([x1+1, x1-1])\n",
    "    u = np.random.uniform()\n",
    "    x2 = x2_tmp if u < alpha(x2_tmp, x1) else x1\n",
    "    x1 = x2\n",
    "    samples3[i] = x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the initial position matters in this problem. If the initial position $x_0$ is very large, $q(x_0)$ will be very close to $q(x_0 -1)$ and $q_(x_0 + 1)$, thus $\\alpha(x_0+1, x_0)$ is also close to 1 ($\\alpha(x_0-1, x_0)$ = 1). This means that the Markov chain could walk to the right easily and takes very long iterations to converge to the true density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(samples3, kde = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True density function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = range(1, 25)\n",
    "plt.bar(X3, 1/np.power(X3, alpha3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consider the joint density $\\pi(x,y)$ and a Gibbs sampling framework is usedto sample a Markov chain with stationary density $\\pi$.  The conditionals are $\\pi(x|y)$ and $\\pi(y|x)$ and while it is easy to sample $\\pi(x|y)$ it is not possible tosample $\\pi(y|x)$ directly. \n",
    "\n",
    "How to proceed using a Metropolis-Hasting step:\n",
    "\n",
    "1. start from an initial $x_0$, $y_0$\n",
    "2. sample $x_1$ from $\\pi(x|y)$ directly\n",
    "3. sample $y_1$ from $\\pi(y|x)$ using one step of Metropolis-Hasting:\n",
    "\n",
    "    i. sample $y^*$ from proposal density $q(y^* | y)$. \n",
    "    \n",
    "    ii. sample u from U(0,1)\n",
    "    \n",
    "    iii. accept $y^*$ if $u < \\alpha(y^*, y_0)$, else stay at $y_0$. In this case:\n",
    "    \\begin{equation}\n",
    "    \\alpha(y_2, y_1) = \\frac{\\pi(y_2 | x_1)}{ \\pi(y_1 | x_1)} \\frac{q(y_1 | y_2)}{ q(y_2 | y_1)}\n",
    "    \\end{equation}\n",
    "    \n",
    "4. repeat 2 and 3 until enough samples are acquired.\n",
    "\n",
    "Why does this work:\n",
    "\n",
    "The trainsition density: $P(x', y' | x, y)=\\pi(x'|y') \\cdot P_M(y' | x, y)$\n",
    "\n",
    "Then, consider $\\pi(x', y')$:\n",
    "\n",
    "\\begin{align}\n",
    "\\int \\int P(x', y' | x, y)\\pi(x, y) \\ dx \\ dy \n",
    "            &= \\int \\int \\pi(x'|y') \\cdot P_M(y' | x, y)\\pi(x, y) \\ dx \\ dy \\\\\n",
    "            &= \\pi(x'|y') \\int \\int P_M(y' | x, y)\\pi(x, y) \\ dx \\ dy \n",
    "\\end{align}\n",
    "\n",
    "Note that $P_M(y' | x, y)\\pi(y | x) = P_M(y | x, y') \\pi(y' | x)$\n",
    "\n",
    "\\begin{align}\n",
    " &= \\pi(x'|y') \\int \\int P_M(y | x, y')\\pi(y' | x) / \\pi(y|x) \\cdot \\pi(y, x) \\ dx \\ dy \\\\\n",
    "            &= \\pi(x'|y') \\int \\int P_M(y | x, y')\\pi(y' | x) \\pi(x) \\ dx \\ dy \\\\\n",
    "            &= \\pi(x'|y') \\pi(y') \\\\\n",
    "            &= \\pi(x', y')\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\pi(x,y) \\propto e^{-4x} y^3 e^{-3ye^x}$. $\\pi(y|x)$ is easy to sample. $y|_x \\sim Gamma(4, 3 e^x) $. However, $\\pi(x|y) \\propto e^{-(4x+3ye^x)}$ would require metropolis-hasting step. Use a lognormal proposal density to do this Metropolis step:\n",
    "\n",
    "\\begin{equation}\n",
    "q(x_2 | x_1) = \\frac{1}{x_2} e^{-(\\log x_2 - \\log x_1)^2/2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pi4(x, y):\n",
    "    return np.exp(-4*x-3*y*np.exp(x))\n",
    "\n",
    "def q4(x2, x1):\n",
    "    return 1/x2 * np.exp(-1/2 * (np.log(x2) - np.log(x1))**2)\n",
    "\n",
    "def alpha4(x2, x1, y):\n",
    "    return min(1, pi4(x2, y) / pi4(x1, y) * x2/x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MH_x(x, y):\n",
    "    x_tmp = np.random.lognormal(np.log(x), 1)\n",
    "    u = np.random.uniform()\n",
    "    x = x_tmp if u < alpha4(x_tmp, x, y) else x\n",
    "    return x\n",
    "\n",
    "def draw_y(x):\n",
    "    return np.random.gamma(shape = 4, scale = 1/3/np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N4 = 100000\n",
    "samples4 = np.zeros((N4, 2))\n",
    "x = 1\n",
    "y = np.random.gamma(4, 5)\n",
    "for i in range(N4):\n",
    "    y = draw_y(x)\n",
    "    x = MH_x(x, y)\n",
    "    samples4[i, 0] = x\n",
    "    samples4[i, 1] = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 100000 iterations, the sample distribution of x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(samples4[:,0], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true marginal density of $x \\propto e^{-8x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,1.2, 1001)\n",
    "plt.plot(x, np.exp(-8*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample distribution of y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(samples4[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the convergence of the Monte Carlo integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I4 = (samples4[:,0] * samples4[:,1])\n",
    "for i in range(1, N4):\n",
    "    I4[i] = (samples4[:i,0]*samples4[:i,1]).mean()\n",
    "    \n",
    "plt.plot(range(N4), I4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the first 20000 steps as burn-in and compute the estimated integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Estimated integration: \",(samples4[20000:,0] * samples4[20000:,1]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This estimation also agrees with the true integrationw which is approximately 0.1317"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
